#! /usr/bin/env python

from transformers import pipeline, set_seed

import os
import sys

class _CaptureStderr(object):
    def __init__(self, stderr=None):
        # self._stdout = stdout or sys.stdout
        self._stderr = stderr or sys.stderr

    def __enter__(self):
        self.old_stderr = sys.stderr
        self.old_stderr.flush()
        sys.stderr = open(os.devnull, 'w')

    def __exit__(self, exc_type, exc_value, traceback):
        self._stderr.flush()
        sys.stderr = self.old_stderr


def _seed_if_necessary(seed):
    if seed is not None:
        set_seed(seed)


def summarization(
        text,
        model='facebook/bart-large-cnn',
        max_length=130,
        min_length=30,
        do_sample=False,
        seed=None
    ):

    _seed_if_necessary(seed)

    pipe = pipeline(task='summarization', model=model)

    return pipe(
        text,
        max_length=max_length,
        min_length=min_length,
        do_sample=do_sample
    )


def text_generation(
        prompt, 
        max_length=200,
        num_return_sequences=3,
        model='small',
        seed=None
    ):
    '''
    Generate text from a prompt.

    Options for model are:
        - 'small', 'medium', 'large' (mapping to distilgpt2, gpt2, and gpt-j-6b)
        - any other text model on HF

    :param prompt: Text to prompt the pipeline with.
    :param model: (optional) Model to use. Default 'small'.
    :param max_length: (optional) Length of text to generate. Default 200.
    :param num_return_sequences: (optional) Number of different responses to make. Default 3.
    :param seed: (optional) Seed value for reproducible builds.

    :returns: A list of text generated by the model.
    '''

    _seed_if_necessary(seed)

    if model in ['small', 'medium', 'large']:
        mapping = {
            'small': 'distilgpt2',
            'medium': 'gpt2',
            'large': 'EleutherAI/gpt-j-6B'
        }
        model = mapping[model]

    with _CaptureStderr():
        pipe = pipeline(task='text-generation', model=model)

        results = pipe(
            prompt,
            max_length=max_length,
            num_return_sequences=num_return_sequences
        )

    return [r['generated_text'] for r in results]


def sentiment_analysis(
        text,
        model='distilbert-base-uncased-finetuned-sst-2-english',
        seed=None
    ):

    _seed_if_necessary(seed)

    pipe = pipeline(task='sentiment-analysis', model=model)

    return pipe(text)


def fill_mask(
        text,
        model='bert-base-uncased',
        seed=None
    ):

    _seed_if_necessary(seed)

    pipe = pipeline(task='fill-mask', model=model)

    return pipe(text)


def question_answering(
        question,
        context,
        model='roberta-base-squad2',
        seed=None
    ):

    _seed_if_necessary(seed)

    pipe = pipeline(task='question_answering', model=model)

    return pipe({
        'question': question,
        'context': context
    })

