#! /usr/bin/env python

import os

# this is nasty but we have to make sure this is loaded before we load in the
# transformers library
def _setup():
    # make sure that the models cache is set up
    if not 'TRANSFORMERS_CACHE' in os.environ:
        if not os.path.exists('./model_cache'):
            os.mkdir('./model_cache')

        os.environ['TRANSFORMERS_CACHE'] = os.path.abspath('./model_cache/')

_setup()


from transformers import pipeline, set_seed # type: ignore
from transformers.utils.logging import set_verbosity_error

from typing import List, Any
from IPython.display import Markdown, display


# silence errors from deep within Transformers
set_verbosity_error()


def _is_notebook() -> bool:
    """
    Try to automatically detect if the code is running in an IPython
    notebook.

    :return: True if it's definitely an IPython notebook.
    """
    try:
        # Attempt to resolve the ipython class name (type: ignore for linter)
        shell = get_ipython().__class__.__name__ # type: ignore

        if shell == 'ZMQInteractiveShell':
            # Jupyter notebook or qtconsole
            return True
        elif shell == 'TerminalInteractiveShell':
            # Terminal running IPython
            return False
        else:
            # Other type (?)
            return False

    except NameError:
        # Probably standard Python interpreter
        return False



def _seed_if_necessary(seed):
    """
    Quick function that sets the seed if it's passed. If not provided,
    no seed will be explicitly set

    :param seed: Seed to set or None
    """
    if seed is not None:
        set_seed(seed)


def _render_output_text(output: List[str]):
    """
    Attempt to import and render the passed text to an IPython shell via nice
    markdown rendering.

    Returns the original input (unchanged) if no notebook is detected.

    :param output: model output (text form)
    :return: either the original output or a transformation into an ipython notebook form
    """

    if not _is_notebook():
        return output

    text = ''
    for i, item in enumerate(output):
        item = item.replace('\n', '\n\n> ')
        text += f'Sample {i+1}:\n\n> {item}\n\n' 

    return display(Markdown(text))


def summarization(
        text,
        model='facebook/bart-large-cnn',
        max_length=130,
        min_length=30,
        do_sample=False,
        seed=None,
        render=True
    ):

    _seed_if_necessary(seed)

    pipe = pipeline(task='summarization', model=model)

    return pipe(
        text,
        max_length=max_length,
        min_length=min_length,
        do_sample=do_sample
    )


def text_generation(
        prompt, 
        max_length=200,
        num_return_sequences=3,
        model='small',
        seed=None,
        render=None
    ):
    '''
    Generate text from a prompt.

    Options for model are:
        - 'small', 'medium', 'large' (mapping to distilgpt2, gpt2, and gpt-j-6b)
        - any other text model on HF

    :param prompt: Text to prompt the pipeline with.
    :param model: (optional) Model to use. Default 'small'.
    :param max_length: (optional) Length of text to generate. Default 200.
    :param num_return_sequences: (optional) Number of different responses to make. Default 3.
    :param seed: (optional) Seed value for reproducible pipeline runs.
    :param render: (optional) Automatically render results for an ipython notebook 
                   if one is detected. Default True

    :returns: A list of text generated by the moder.
    '''

    _seed_if_necessary(seed)

    if model in ['small', 'medium', 'large']:
        mapping = {
            'small': 'distilgpt2',
            'medium': 'gpt2',
            'large': 'EleutherAI/gpt-j-6B'
        }
        model = mapping[model]

    pipe = pipeline(task='text-generation', model=model)

    results = pipe(
        prompt,
        max_length=max_length,
        num_return_sequences=num_return_sequences
    )

    # convert results to a list of strings
    results = [r['generated_text'] for r in results]

    # skip render attempt if not requested
    if not render:
        return results

    return _render_output_text(results)



def sentiment_analysis(
        text,
        model='distilbert-base-uncased-finetuned-sst-2-english',
        seed=None,
        render=False
    ):

    _seed_if_necessary(seed)

    pipe = pipeline(task='sentiment-analysis', model=model)

    return pipe(text)


def fill_mask(
        text,
        model='bert-base-uncased',
        seed=None,
        render=False
    ):

    _seed_if_necessary(seed)

    pipe = pipeline(task='fill-mask', model=model)

    return pipe(text)


def question_answering(
        question,
        context,
        model='roberta-base-squad2',
        seed=None,
        render=False
    ):

    _seed_if_necessary(seed)

    pipe = pipeline(task='question_answering', model=model)

    return pipe({
        'question': question,
        'context': context
    })

